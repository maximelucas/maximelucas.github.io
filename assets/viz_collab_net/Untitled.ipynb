{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7886e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some options\n",
    "inputBibFileName = \"../../_bibliography/papers.bib\"\n",
    "outputJSONFileName = \"collab_net.json\"\n",
    "#\n",
    "authorInformationFile = (\n",
    "    \"\"  #'authorinfoRosalindFranklin.csv' # optional co-author information\n",
    ")\n",
    "deleteEgoNode = False\n",
    "##\n",
    "\n",
    "import csv  # for loading comma seperated values\n",
    "import json  # for writing the json\n",
    "import re\n",
    "\n",
    "# import necessary libraries\n",
    "from pybtex.database.input import bibtex  # for reading the bib files\n",
    "\n",
    "\n",
    "# some auxiliary functions\n",
    "# you migth have to add further repalcement rules\n",
    "def latex2unicode(latexString):\n",
    "    \"\"\"takes the name of an author as string and return the string\n",
    "    with latex character replaced as normal string for the HTML\"\"\"\n",
    "    latexString = latexString.replace('{\\\\\"u}', \"ü\")\n",
    "    latexString = latexString.replace(\"{\\\\'o}\", \"ó\")\n",
    "    latexString = latexString.replace(\"{\\\\'a}\", \"á\")\n",
    "    latexString = latexString.replace(\"{\\\\~a}\", \"ã\")\n",
    "\n",
    "    return latexString\n",
    "\n",
    "\n",
    "# some preperation to read the bibtex file\n",
    "parser = bibtex.Parser()\n",
    "bib_data = parser.parse_file(inputBibFileName)\n",
    "\n",
    "\n",
    "listOfAuthors = []  # empty list of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d30fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Duncan, R.', 'Lucas, M.']\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "for paperKeys in bib_data.entries.keys():\n",
    "    # convert author names to strings and remove * for co-first\n",
    "    thors = [latex2unicode(str(author)) for author in bib_data.entries[paperKeys].persons[\"author\"] ]\n",
    "    thors = [author.replace(\"*\", \"\") for author in thors]\n",
    "    bib_data.entries[paperKeys].persons[\"author\"] = thors\n",
    "    # remove latex formatting from titles \"{}\"\n",
    "    title = bib_data.entries[paperKeys].fields[\"title\"]\n",
    "    title = title.replace(\"{\", \"\")\n",
    "    title = title.replace(\"}\", \"\")\n",
    "    bib_data.entries[paperKeys].fields[\"title\"] = title\n",
    "    \n",
    "print(bib_data.entries[paperKeys].persons[\"author\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34224a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moriamé, M.',\n",
       " 'Lucas, M.',\n",
       " 'Carletti, T.',\n",
       " 'Lucas, M.',\n",
       " 'Gallo, L.',\n",
       " 'Ghavasieh, A.',\n",
       " 'Battiston, F.',\n",
       " 'De Domenico, M.',\n",
       " 'Nurisso, M.',\n",
       " 'Morandini, M.',\n",
       " 'Lucas, M.',\n",
       " 'Vaccarino, F.',\n",
       " 'Gili, T.',\n",
       " 'Petri, G.',\n",
       " 'Robiglio, T.',\n",
       " 'Neri, M.',\n",
       " 'Coppes, D.',\n",
       " 'Agostinelli, C.',\n",
       " 'Battiston, F.',\n",
       " 'Lucas, M.',\n",
       " 'Petri, G.',\n",
       " 'Santoro, A.',\n",
       " 'Battiston, F.',\n",
       " 'Lucas, M.',\n",
       " 'Petri, G.',\n",
       " 'Amico, E.',\n",
       " 'Zhang, Y.',\n",
       " 'Skardal, P. S.',\n",
       " 'Battiston, F.',\n",
       " 'Petri, G.',\n",
       " 'Lucas, M.',\n",
       " 'Brondetta, A.',\n",
       " 'Bizyaeva, A.',\n",
       " 'Lucas, M.',\n",
       " 'Petri, G.',\n",
       " 'Musslick, S.',\n",
       " 'Leitão, A.',\n",
       " 'Lucas, M.',\n",
       " 'Poetto, S.',\n",
       " 'Hersh, T. A.',\n",
       " 'Gero, S.',\n",
       " 'Gruber, D.',\n",
       " 'Bronstein, M.',\n",
       " 'Petri, G.',\n",
       " 'Nurisso, M.',\n",
       " 'Arnaudon, A.',\n",
       " 'Lucas, M.',\n",
       " 'Peach, R. L.',\n",
       " 'Expert, P.',\n",
       " 'Vaccarino, F.',\n",
       " 'Petri, G.',\n",
       " 'Lucas, M.',\n",
       " 'Townsend-Teague, A.',\n",
       " 'Neri, M.',\n",
       " 'Poetto, S.',\n",
       " 'Morris, A.',\n",
       " 'Habermann, B. H.',\n",
       " 'Tichit, L.',\n",
       " 'Landry, N. W.',\n",
       " 'Lucas, M.',\n",
       " 'Iacopini, I.',\n",
       " 'Petri, G.',\n",
       " 'Schwarze, A.',\n",
       " 'Patania, A.',\n",
       " 'Torres, L.',\n",
       " 'Zhang, Y.',\n",
       " 'Lucas, M.',\n",
       " 'Battiston, F.',\n",
       " 'Lucas, M.',\n",
       " 'Iacopini, I.',\n",
       " 'Robiglio, T.',\n",
       " 'Barrat, A.',\n",
       " 'Petri, G.',\n",
       " 'Lucas, M.',\n",
       " 'Morris, A.',\n",
       " 'Townsend-Teague, A.',\n",
       " 'Tichit, L.',\n",
       " 'Habermann, B. H.',\n",
       " 'Barrat, A.',\n",
       " 'Lucas, M.',\n",
       " 'Cencetti, G.',\n",
       " 'Battiston, F.',\n",
       " 'Newman, J.',\n",
       " 'Lucas, M.',\n",
       " 'Stefanovska, A.',\n",
       " 'Lucas, M.',\n",
       " 'Newman, J.',\n",
       " 'Stefanovska, A.',\n",
       " 'Newman, J.',\n",
       " 'Lucas, M.',\n",
       " 'Stefanovska, A.',\n",
       " 'Lucas, M.',\n",
       " 'Cencetti, G.',\n",
       " 'Battiston, F.',\n",
       " 'Battiston, F.',\n",
       " 'Cencetti, G.',\n",
       " 'Iacopini, I.',\n",
       " 'Latora, V.',\n",
       " 'Lucas, M.',\n",
       " 'Patania, A.',\n",
       " 'Young, J.-G.',\n",
       " 'Petri, G.',\n",
       " 'Lucas, M.',\n",
       " 'Lucas, M.',\n",
       " 'Fanelli, D.',\n",
       " 'Stefanovska, A.',\n",
       " 'Lucas, M.',\n",
       " 'Fanelli, D.',\n",
       " 'Carletti, T.',\n",
       " 'Petit, J.',\n",
       " 'Lucas, M.',\n",
       " 'Newman, J.',\n",
       " 'Stefanovska, A.',\n",
       " 'Lucas, M.',\n",
       " 'Lucas, M.',\n",
       " 'Duncan, R.',\n",
       " 'Lucas, M.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfAuthors = []  # empty list of authors\n",
    "\n",
    "# go throuh all entries\n",
    "for paperKeys in bib_data.entries.keys():\n",
    "    # get the authors of this paper\n",
    "\n",
    "    # authors = bib_data.entries[paperKeys].persons['author'].split(\" and \")\n",
    "    # save them to the list of authors\n",
    "    for author in bib_data.entries[paperKeys].persons[\"author\"]:\n",
    "        listOfAuthors.append(str(author))\n",
    "\n",
    "listOfAuthors\n",
    "\n",
    "# remove * for co-first \n",
    "#listOfAuthors = [author.replace(\"*\", \"\") for author in listOfAuthors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "811c6c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no optional co-author information available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if deleteEgoNode == True:\n",
    "    # we assume that the author with the most entries is the ego node and delete it\n",
    "    egoNode = max(listOfAuthors, key=listOfAuthors.count)  # returns the ego node\n",
    "    listOfAuthors = list(set(listOfAuthors))  # gets unique list of authors\n",
    "    listOfAuthors.pop(\n",
    "        listOfAuthors.index(egoNode)\n",
    "    )  # deletes it from the list of authors\n",
    "    print(\"Removing the ego node: %s \" % egoNode)\n",
    "else:\n",
    "    listOfAuthors = list(set(listOfAuthors))  # gets unique list of authors\n",
    "\n",
    "nAuthors = len(listOfAuthors)  # number of author nodes\n",
    "nPapers = len(bib_data.entries.keys())  # number of paper nodes\n",
    "\n",
    "\n",
    "# read the additional author information from the csv\n",
    "authorLinks_dict = {}  # create an empty dictionary\n",
    "authorImage_dict = {}  # create an empty dictionary\n",
    "\n",
    "try:\n",
    "    authorInfo_reader = csv.DictReader(open(authorInformationFile))\n",
    "    for row in authorInfo_reader:\n",
    "        authorLinks_dict[row[\"name\"]] = row[\"url\"]\n",
    "        authorImage_dict[row[\"name\"]] = row[\"image\"]\n",
    "except FileNotFoundError:\n",
    "    print(\"no optional co-author information available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00d8bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L. Gallo\n",
      "A. Ghavasieh\n",
      "A. Brondetta\n",
      "M. Bronstein\n",
      "A. Bizyaeva\n",
      "I. Iacopini\n",
      "T. Robiglio\n",
      "J. Petit\n",
      "L. Torres\n",
      "R. L. Peach\n",
      "P. S. Skardal\n",
      "T. A. Hersh\n",
      "J. Newman\n",
      "A. Leitão\n",
      "D. Gruber\n",
      "P. Expert\n",
      "D. Coppes\n",
      "A. Morris\n",
      "B. H. Habermann\n",
      "T. Gili\n",
      "M. Neri\n",
      "V. Latora\n",
      "Y. Zhang\n",
      "S. Musslick\n",
      "D. Fanelli\n",
      "T. Carletti\n",
      "A. Townsend-Teague\n",
      "F. Battiston\n",
      "A. Santoro\n",
      "E. Amico\n",
      "M. Morandini\n",
      "A. Arnaudon\n",
      "A. Patania\n",
      "L. Tichit\n",
      "A. Stefanovska\n",
      "M. Lucas\n",
      "J.-G. Young\n",
      "R. Duncan\n",
      "C. Agostinelli\n",
      "A. Schwarze\n",
      "M. De Domenico\n",
      "S. Poetto\n",
      "S. Gero\n",
      "G. Cencetti\n",
      "M. Nurisso\n",
      "F. Vaccarino\n",
      "N. W. Landry\n",
      "M. Moriamé\n",
      "G. Petri\n",
      "A. Barrat\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary reflecting the graph (there are more pythonic ways\n",
    "# possible to creat this, e.g., with zip, but this is easiest)\n",
    "\n",
    "node_list = []\n",
    "# create author nodes\n",
    "for i in range(nAuthors):\n",
    "    node_dict = {}  # create an empty dictionary for this node\n",
    "    node_dict[\"id\"] = \"A\" + str(i)\n",
    "    node_dict[\"group\"] = 0\n",
    "    # invert the name such that the given name is before the last name\n",
    "    try:  # we need this try to deal with single author papers\n",
    "        authorSplit = listOfAuthors[i].split(\",\")\n",
    "        nameThisAuthor = authorSplit[1][1::] + \" \" + authorSplit[0]\n",
    "    except:\n",
    "        authorSplit = listOfAuthors[i]\n",
    "        nameThisAuthor = authorSplit[1][1::] + \" \" + authorSplit[0]\n",
    "\n",
    "    nameThisAuthorUnicode = latex2unicode(nameThisAuthor)\n",
    "    print(nameThisAuthorUnicode)\n",
    "    node_dict[\"name\"] = nameThisAuthorUnicode\n",
    "    node_list.append(node_dict)\n",
    "    # try to set the url for this author but default is google it\n",
    "    node_dict[\"url\"] = \"https://www.google.com/search?q=\" + nameThisAuthor\n",
    "    try:\n",
    "        if authorLinks_dict[nameThisAuthor] is None:\n",
    "            raise KeyError(\"no information for this author\")\n",
    "        else:\n",
    "            node_dict[\"url\"] = authorLinks_dict[nameThisAuthor]\n",
    "    except KeyError:\n",
    "        node_dict[\"url\"] = \"https://www.google.com/search?q=\" + nameThisAuthor\n",
    "\n",
    "        # try to set a image for this author\n",
    "    try:\n",
    "        node_dict[\"image\"] = authorImage_dict[nameThisAuthor]\n",
    "    except KeyError:  # if no image jsut leave blank\n",
    "        node_dict[\"image\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e131735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the links between the nodes\n",
    "link_list = []\n",
    "i=0\n",
    "for paperKeys in bib_data.entries.keys(): # go over every paper\n",
    "    # create the paper node\n",
    "    node_dict = {} # create an empty dictionary for this node\n",
    "    node_dict[\"id\"] = \"P\" + str(i)\n",
    "    node_dict[\"group\"] = 1\n",
    "    thisPaperName = bib_data.entries[paperKeys].fields['title']\n",
    "    # remove curly bracket in paper name, remove it\n",
    "    if thisPaperName[0]=='{':\n",
    "        thisPaperName=thisPaperName[1:-1]\n",
    "    node_dict[\"name\"] =  thisPaperName\n",
    "    node_list.append(node_dict)\n",
    "\n",
    "    # set image for this paper\n",
    "    try:\n",
    "        node_dict[\"image\"] = bib_data.entries[paperKeys].fields['image']\n",
    "    except KeyError: # if no image just leave blank\n",
    "        node_dict[\"image\"] = []\n",
    "\n",
    "\n",
    "    # find the authors for this paper\n",
    "    authorsThisPaper = [str(i).split('*')[0].strip() for i in bib_data.entries[paperKeys].persons['author']]\n",
    "\n",
    "    # if the paper has a url, add it\n",
    "    try:\n",
    "        node_dict[\"url\"] = bib_data.entries[paperKeys].fields['url']\n",
    "    except KeyError: # otherwise refer to google\n",
    "        node_dict[\"url\"] = \"https://www.google.com/search?q=\" + bib_data.entries[paperKeys].fields['title']\n",
    "\n",
    "\n",
    "    for authors in authorsThisPaper:\n",
    "        link_dict = {} # empty dictionary for this edge\n",
    "        link_dict[\"source\"] = \"P\" + str(i) # attached to this paper\n",
    "        try:\n",
    "            link_dict[\"target\"] = \"A\" + str(listOfAuthors.index(authors)) # and attached to co-author\n",
    "            link_list.append(link_dict)    # save it into the list\n",
    "        except ValueError:\n",
    "            pass\n",
    "            #print(\"Author %s not in list, probably the ego node.\" %authors )\n",
    "    i=i+1\n",
    "\n",
    "# write into dictionary\n",
    "graph_dict = {\"nodes\" : node_list, \"links\" : link_list}\n",
    "\n",
    "\n",
    "# opening the file to write\n",
    "if outputJSONFileName:\n",
    "    # Writing JSON data\n",
    "    with open(outputJSONFileName, 'w') as f:\n",
    "        json.dump(graph_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60c119dc-4968-48bd-b2b4-f6624707f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'A35', 'group': 0, 'name': 'M. Lucas', 'url': 'https://www.google.com/search?q=M. Lucas', 'image': []}\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(node_list):\n",
    "    if node[\"name\"] == \"M. Lucas\":\n",
    "        idx = node[\"id\"]\n",
    "        ii = i\n",
    "        print(node)\n",
    "        break\n",
    "    else: \n",
    "        ii=None\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a751d45-83c5-43e5-b0c5-a28615dde9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'A35',\n",
       " 'group': 0,\n",
       " 'name': 'M. Lucas',\n",
       " 'url': 'https://www.google.com/search?q=M. Lucas',\n",
       " 'image': []}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove node representing myself\n",
    "node_list.pop(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3182c08f-fc6c-4a17-877c-655f9362e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links including me\n",
    "link_list = [i for i in link_list if idx not in i.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b645663-11d0-4e72-8861-e8bb7fe2b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file to write\n",
    "if outputJSONFileName:\n",
    "    # Writing JSON data\n",
    "    with open(outputJSONFileName, 'w') as f:\n",
    "        json.dump(graph_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473182c2-329b-4a33-a2a1-71114111c561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b707010f-dacf-4a02-8275-6d5f4b63a57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'P0', 'target': 'A47'},\n",
       " {'source': 'P0', 'target': 'A25'},\n",
       " {'source': 'P1', 'target': 'A0'},\n",
       " {'source': 'P1', 'target': 'A1'},\n",
       " {'source': 'P1', 'target': 'A27'},\n",
       " {'source': 'P1', 'target': 'A40'},\n",
       " {'source': 'P2', 'target': 'A44'},\n",
       " {'source': 'P2', 'target': 'A30'},\n",
       " {'source': 'P2', 'target': 'A45'},\n",
       " {'source': 'P2', 'target': 'A19'},\n",
       " {'source': 'P2', 'target': 'A48'},\n",
       " {'source': 'P3', 'target': 'A6'},\n",
       " {'source': 'P3', 'target': 'A20'},\n",
       " {'source': 'P3', 'target': 'A16'},\n",
       " {'source': 'P3', 'target': 'A38'},\n",
       " {'source': 'P3', 'target': 'A27'},\n",
       " {'source': 'P3', 'target': 'A48'},\n",
       " {'source': 'P4', 'target': 'A28'},\n",
       " {'source': 'P4', 'target': 'A27'},\n",
       " {'source': 'P4', 'target': 'A48'},\n",
       " {'source': 'P4', 'target': 'A29'},\n",
       " {'source': 'P5', 'target': 'A22'},\n",
       " {'source': 'P5', 'target': 'A10'},\n",
       " {'source': 'P5', 'target': 'A27'},\n",
       " {'source': 'P5', 'target': 'A48'},\n",
       " {'source': 'P6', 'target': 'A2'},\n",
       " {'source': 'P6', 'target': 'A4'},\n",
       " {'source': 'P6', 'target': 'A48'},\n",
       " {'source': 'P6', 'target': 'A23'},\n",
       " {'source': 'P7', 'target': 'A13'},\n",
       " {'source': 'P7', 'target': 'A41'},\n",
       " {'source': 'P7', 'target': 'A11'},\n",
       " {'source': 'P7', 'target': 'A42'},\n",
       " {'source': 'P7', 'target': 'A14'},\n",
       " {'source': 'P7', 'target': 'A3'},\n",
       " {'source': 'P7', 'target': 'A48'},\n",
       " {'source': 'P8', 'target': 'A44'},\n",
       " {'source': 'P8', 'target': 'A31'},\n",
       " {'source': 'P8', 'target': 'A9'},\n",
       " {'source': 'P8', 'target': 'A15'},\n",
       " {'source': 'P8', 'target': 'A45'},\n",
       " {'source': 'P8', 'target': 'A48'},\n",
       " {'source': 'P9', 'target': 'A26'},\n",
       " {'source': 'P9', 'target': 'A20'},\n",
       " {'source': 'P9', 'target': 'A41'},\n",
       " {'source': 'P9', 'target': 'A17'},\n",
       " {'source': 'P9', 'target': 'A18'},\n",
       " {'source': 'P9', 'target': 'A33'},\n",
       " {'source': 'P10', 'target': 'A46'},\n",
       " {'source': 'P10', 'target': 'A5'},\n",
       " {'source': 'P10', 'target': 'A48'},\n",
       " {'source': 'P10', 'target': 'A39'},\n",
       " {'source': 'P10', 'target': 'A32'},\n",
       " {'source': 'P10', 'target': 'A8'},\n",
       " {'source': 'P11', 'target': 'A22'},\n",
       " {'source': 'P11', 'target': 'A27'},\n",
       " {'source': 'P12', 'target': 'A5'},\n",
       " {'source': 'P12', 'target': 'A6'},\n",
       " {'source': 'P12', 'target': 'A49'},\n",
       " {'source': 'P12', 'target': 'A48'},\n",
       " {'source': 'P13', 'target': 'A17'},\n",
       " {'source': 'P13', 'target': 'A26'},\n",
       " {'source': 'P13', 'target': 'A33'},\n",
       " {'source': 'P13', 'target': 'A18'},\n",
       " {'source': 'P13', 'target': 'A49'},\n",
       " {'source': 'P14', 'target': 'A43'},\n",
       " {'source': 'P14', 'target': 'A27'},\n",
       " {'source': 'P15', 'target': 'A12'},\n",
       " {'source': 'P15', 'target': 'A34'},\n",
       " {'source': 'P16', 'target': 'A12'},\n",
       " {'source': 'P16', 'target': 'A34'},\n",
       " {'source': 'P17', 'target': 'A12'},\n",
       " {'source': 'P17', 'target': 'A34'},\n",
       " {'source': 'P18', 'target': 'A43'},\n",
       " {'source': 'P18', 'target': 'A27'},\n",
       " {'source': 'P19', 'target': 'A27'},\n",
       " {'source': 'P19', 'target': 'A43'},\n",
       " {'source': 'P19', 'target': 'A5'},\n",
       " {'source': 'P19', 'target': 'A21'},\n",
       " {'source': 'P19', 'target': 'A32'},\n",
       " {'source': 'P19', 'target': 'A36'},\n",
       " {'source': 'P19', 'target': 'A48'},\n",
       " {'source': 'P21', 'target': 'A24'},\n",
       " {'source': 'P21', 'target': 'A34'},\n",
       " {'source': 'P22', 'target': 'A24'},\n",
       " {'source': 'P22', 'target': 'A25'},\n",
       " {'source': 'P22', 'target': 'A7'},\n",
       " {'source': 'P23', 'target': 'A12'},\n",
       " {'source': 'P23', 'target': 'A34'},\n",
       " {'source': 'P26', 'target': 'A37'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19268ca6-80e4-4f40-b3a5-438b837ed327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36cb56-cf6a-4400-9486-ca7dbcc52251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47e50b-a6ed-4cf6-a835-fe022d0a5d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6d98-8060-496e-9715-400ed358e058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da4330-047c-4219-8497-4e2ed430506b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6eb63-fc97-4399-b895-a12a28f0d237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_xgi",
   "language": "python",
   "name": "venv_xgi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
